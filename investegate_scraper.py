#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Investegate scraper + investor-grade filter
STRICT mode:
- Only announcements with user_score >= 1 (i.e., match YOUR phrases) are kept for outputs and AI.
- Generic triggers still compute 'score' (for ordering) but do NOT include items by themselves.

Outputs (only user matches are shown):
  out/YYYY-MM-DD/investegate_raw.json        (all scanned items kept by since_days, for reference)
  out/YYYY-MM-DD/investegate_hits.csv        (ONLY user_score >= 1)
  out/YYYY-MM-DD/investegate_summary.md      (ONLY user_score >= 1)
  out/YYYY-MM-DD/site/index.html             (ONLY user_score >= 1, inline JSON; double-click works)

Run examples:
  python investegate_scraper.py --pages 2 --per_page 300 --since_days 7 --keywords_file keywords.txt --use_ai true
"""

import os, re, json, csv, time, argparse, pathlib, html
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Any, Optional
from urllib.parse import urljoin
import requests
from bs4 import BeautifulSoup
from dateutil import parser as dtparse

# ----------------------------
# Config
# ----------------------------
DEFAULT_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) "
                  "Chrome/120.0 Safari/537.36"
}
REQUEST_TIMEOUT = 20
SLEEP_BETWEEN_ITEM_SEC = 0.8

# Broad investor triggers (used for extra ranking/context, NOT for inclusion)
BUILTIN_INVESTOR_TRIGGERS = [
    r"\b(merger|acquisition|acquire|disposal|divest|scheme of arrangement|takeover|bid)\b",
    r"\b(placing|fundraise|equity placing|rights issue|open offer|private placement)\b",
    r"\b(buyback|repurchase|tender offer)\b",
    r"\b(profit warning|trading update|guidance|downgrade|upgrade|outlook)\b",
    r"\b(earnings|EBITDA|free cash flow|FCF|cash flow|margin|gross margin|operating margin)\b",
    r"\b(leverage|net debt|liquidity|covenant|going concern|refinancing|insolvency|administration|bankruptcy)\b",
    r"\b(contract( win| award| extension)?|framework|order book|backlog|LOI|MOU|supply agreement)\b",
    r"\b(capex|commissioning|production ramp|shutdown|outage|incident|recall)\b",
    r"\b(CEO|CFO|chair|resignation|steps down|appointment|board change)\b",
    r"\b(TR-1|holding\(s\) in company|major holdings|13D|13G)\b",
    r"\b(dividend|special dividend|distribution|capital return)\b",
]

# ----------------------------
# Inline mini-site (bullets hidden when AI present)
# ----------------------------
HTML_TEMPLATE = """<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width,initial-scale=1"/>
<title>Investegate — Filtered RNS Digest</title>
<link rel="stylesheet" href="styles.css">
<!-- DATA JSON INJECTED ABOVE </head> -->
</head>
<body>
<header>
  <h1>Investegate — Filtered RNS Digest</h1>
  <div class="meta" id="meta"></div>
  <div class="controls">
    <input id="search" type="text" placeholder="Search title/company/body…"/>
    <select id="sort">
      <option value="score_desc">Sort: Score (high→low)</option>
      <option value="date_desc">Sort: Newest</option>
      <option value="date_asc">Sort: Oldest</option>
      <option value="score_asc">Sort: Score (low→high)</option>
    </select>
  </div>
</header>
<main id="list"></main>
<footer>
  <p>Local view generated by your scraper. No tracking. Data stays on your machine.</p>
</footer>

<script>
(function(){
  const payload = JSON.parse(document.getElementById('data-json').textContent);

  const metaEl = document.getElementById('meta');
  metaEl.textContent = `Scanned: ${payload.stats.pages} × ${payload.stats.per_page} • Min score: ${payload.stats.min_score} • Keywords: ${payload.stats.keywords || '—'} • AI: ${payload.stats.ai_enabled ? 'ON' : 'OFF'} • Showing only your matches (${payload.stats.visible_count})`;

  const searchEl = document.getElementById('search');
  const sortEl = document.getElementById('sort');
  const listEl = document.getElementById('list');

  let rows = payload.rows.slice();

  function render() {
    const q = (searchEl.value || '').toLowerCase();
    let data = rows.filter(r => {
      if (!q) return true;
      const hay = (r.title + ' ' + (r.company||'') + ' ' + (r.body||'') + ' ' + (r.ai_summary||'')).toLowerCase();
      return hay.includes(q);
    });

    const sort = sortEl.value;
    data.sort((a, b) => {
      const ad = a.dt_iso || '';
      const bd = b.dt_iso || '';
      if (sort === 'date_desc') return bd.localeCompare(ad);
      if (sort === 'date_asc') return ad.localeCompare(bd);
      if (sort === 'score_desc') return (b.score||0) - (a.score||0);
      if (sort === 'score_asc') return (a.score||0) - (b.score||0);
      return 0;
    });

    listEl.innerHTML = '';
    for (const r of data) {
      const card = document.createElement('article');
      card.className = 'card';

      const meta = [r.dt_iso ? new Date(r.dt_iso).toLocaleString() : '', r.source, r.company].filter(Boolean).join(' | ');
      const bullets = (r.bullets||[]).map(b => `<li>${b}</li>`).join('');
      const showBullets = !r.ai_summary && bullets;

      card.innerHTML = `
        <h2>${escapeHtml(r.title||'(no title)')}</h2>
        <div class="sub">${escapeHtml(meta)}</div>
        <div class="tags"><span class="score">Score: ${r.score} <span class="subscore">(you:${r.user_score}, trig:${r.trigger_score})</span></span></div>
        ${showBullets ? `<ul class="bullets">${showBullets}</ul>` : ''}
        ${r.ai_summary ? `<div class="ai"><div class="badge">AI</div><p>${escapeHtml(r.ai_summary)}</p></div>` : ''}
        <div class="actions">
          <a href="${r.url}" target="_blank" rel="noopener">Read the announcement</a>
        </div>
      `;
      listEl.appendChild(card);
    }
  }

  function escapeHtml(s){ return (s||'').replace(/[&<>"']/g, m => ({'&':'&amp;','<':'&lt;','>':'&gt;','"':'&quot;',"'":'&#39;'}[m])); }

  searchEl.addEventListener('input', render);
  sortEl.addEventListener('change', render);

  render();
})();
</script>
</body>
</html>
"""

CSS_STYLES = """
:root { --bg:#0b0c10; --panel:#11131a; --muted:#a9b0be; --text:#e8eef7; --accent:#4aa3ff; --accent2:#3bd3a8; }
*{box-sizing:border-box}
body{margin:0;background:var(--bg);color:var(--text);font:16px/1.4 system-ui,Segoe UI,Roboto,Helvetica,Arial}
header{padding:20px;position:sticky;top:0;background:linear-gradient(180deg, rgba(11,12,16,0.98), rgba(11,12,16,0.92))}
h1{margin:0 0 6px 0;font-size:22px}
.meta{color:var(--muted);font-size:14px;margin-bottom:10px}
.controls{display:flex;gap:10px;flex-wrap:wrap}
.controls input[type="text"]{flex:1;min-width:240px;padding:10px;border:1px solid #222;border-radius:10px;background:var(--panel);color:var(--text)}
.controls select{padding:10px;border:1px solid #222;border-radius:10px;background:var(--panel);color:var(--text)}
main{padding:20px;display:grid;gap:14px;grid-template-columns:repeat(auto-fill,minmax(360px,1fr))}
.card{background:var(--panel);border:1px solid #1b1f2a;border-radius:14px;padding:16px;box-shadow:0 1px 2px rgba(0,0,0,.25)}
.card h2{margin:0 0 6px 0;font-size:18px}
.sub{color:var(--muted);font-size:12px;margin-bottom:8px}
.tags{display:flex;align-items:center;gap:8px;margin-bottom:8px}
.badge{display:inline-block;background:linear-gradient(90deg,var(--accent),var(--accent2));color:#001018;font-weight:700;font-size:10px;padding:3px 8px;border-radius:999px}
.subscore{color:var(--muted);font-size:11px;margin-left:6px}
.bullets{margin:8px 0 0 18px}
.ai{margin-top:10px;padding:10px;border:1px dashed #2a3245;border-radius:12px;background:#0e1119}
.actions{display:flex;align-items:center;justify-content:space-between;margin-top:12px}
.actions a{color:var(--accent);text-decoration:none}
.actions a:hover{text-decoration:underline}
footer{padding:20px;color:var(--muted);text-align:center}
"""

# ----------------------------
# Helpers
# ----------------------------
def ensure_dir(p: pathlib.Path) -> None:
    p.mkdir(parents=True, exist_ok=True)

def clean_text(s: str) -> str:
    s = html.unescape(s or "")
    s = re.sub(r"\s+\n", "\n", s)
    s = re.sub(r"\n{3,}", "\n\n", s)
    s = re.sub(r"[ \t]{2,}", " ", s)
    return s.strip()

def fetch(url: str, session: requests.Session) -> Optional[str]:
    try:
        r = session.get(url, headers=DEFAULT_HEADERS, timeout=REQUEST_TIMEOUT)
        if r.status_code == 200:
            return r.text
        return None
    except requests.RequestException:
        return None

def parse_list_page(html_text: str) -> List[Dict[str, str]]:
    soup = BeautifulSoup(html_text, "lxml")
    rows = []
    for a in soup.select('a[href*="/announcement/"]'):
        href = a.get("href", "").strip()
        if not href:
            continue
        href_abs = urljoin("https://www.investegate.co.uk", href)
        headline = a.get_text(strip=True)

        time_str, source, company = None, None, None
        prev = a
        seen = {"time": False, "source": False, "company": False}
        for _ in range(12):
            if not prev: break
            prev = prev.previous_sibling or prev.parent
            if not prev: break
            txt = ""
            if hasattr(prev, "get_text"):
                txt = prev.get_text(" ", strip=True)
            elif isinstance(prev, str):
                txt = prev.strip()
            if not txt: continue
            if not seen["time"] and re.search(r"\b\d{2} \w{3} \d{4} \d{2}:\d{2} (AM|PM)\b", txt):
                time_str = re.search(r"\d{2} \w{3} \d{4} \d{2}:\d{2} (AM|PM)", txt).group(0); seen["time"]=True; continue
            if not seen["source"] and re.search(r"\b(RNS|EQS|PRN|GNW|MFN|FNW)\b", txt):
                source = re.search(r"\b(RNS|EQS|PRN|GNW|MFN|FNW)\b", txt).group(1); seen["source"]=True; continue
            if not seen["company"] and len(txt) <= 120 and len(txt.split()) >= 1:
                company = txt; seen["company"]=True
            if all(seen.values()): break

        rows.append({
            "time_str": time_str or "",
            "source": source or "",
            "company": company or "",
            "headline": headline,
            "href": href_abs,
        })

    # de-dup by href
    seen_hrefs, out = set(), []
    for r in rows:
        if r["href"] in seen_hrefs: continue
        seen_hrefs.add(r["href"]); out.append(r)
    return out

def parse_detail_page(html_text: str) -> Dict[str, Any]:
    soup = BeautifulSoup(html_text, "lxml")
    h1 = soup.find("h1")
    title = h1.get_text(strip=True) if h1 else ""
    parts = []
    for tag in soup.find_all(["p", "li", "h2", "h3", "pre"]):
        txt = tag.get_text(" ", strip=True)
        if txt: parts.append(txt)
    body = clean_text("\n".join(parts))
    date_match = re.search(r"\b(\d{1,2}\s+\w{3,}\s+\d{4})\b", soup.get_text(" ", strip=True))
    dt_iso = None
    if date_match:
        try:
            dt_iso = dtparse.parse(date_match.group(1), dayfirst=True).isoformat()
        except Exception:
            dt_iso = None
    return {"title": title, "body": body, "dt_iso": dt_iso}

# ----------------------------
# Keyword logic
# ----------------------------
def load_keywords(keywords_csv: str, keywords_file: Optional[str]) -> List[str]:
    kws: List[str] = []
    if keywords_csv:
        kws.extend([k.strip() for k in keywords_csv.split(",") if k.strip()])
    if keywords_file and os.path.isfile(keywords_file):
        with open(keywords_file, "r", encoding="utf-8") as f:
            for line in f:
                s = line.strip()
                if s and not s.startswith("#"):
                    kws.append(s)
    # de-dup, preserve order
    seen, out = set(), []
    for k in kws:
        kl = k.lower()
        if kl not in seen:
            seen.add(kl); out.append(k)
    return out

def compile_phrase_patterns(user_keywords: List[str]) -> List[re.Pattern]:
    pats = []
    for kw in user_keywords:
        tokens = re.split(r"\s+", kw.strip())
        parts = [re.escape(t) for t in tokens if t]
        if not parts: continue
        pattern = r"\b" + r"\s+".join(parts) + r"\b"
        try:
            pats.append(re.compile(pattern, flags=re.IGNORECASE))
        except re.error:
            pats.append(re.compile(re.escape(kw), flags=re.IGNORECASE))
    return pats

def count_matches(text: str, patterns: List[re.Pattern]) -> int:
    c = 0
    for rx in patterns:
        if rx.search(text): c += 1
    return c

def extract_bullets(text: str, user_kw_patterns: List[re.Pattern], max_bullets: int = 2) -> List[str]:
    """
    Clean bullets:
    - only sentences hitting YOUR phrases
    - skip term-sheet/legal boilerplate
    - trim long lines
    """
    if not text:
        return []
    sents = re.split(r"(?<=[\.\?\!])\s+(?=[A-Z0-9(])", text)
    sents = [s.strip() for s in sents if len(s.strip()) >= 20]

    SKIP_PHRASES = [
        "base prospectus","final terms","securities programme","tranche","medium term note","isin",
        "custodian","fixing time","reference level","bloomberg screen","denomination","coupon"
    ]
    def too_numeric(s: str) -> bool:
        digits = sum(c.isdigit() for c in s)
        return digits / max(1, len(s)) > 0.25

    def user_hit(s: str) -> bool:
        return any(rx.search(s) for rx in user_kw_patterns)

    clean = []
    for s in sents:
        low = s.lower()
        if any(p in low for p in SKIP_PHRASES):
            continue
        if too_numeric(s):
            continue
        if not user_hit(s):
            continue
        if len(s) > 220:
            s = s[:217].rstrip() + "…"
        clean.append(s)

    if not clean:
        # Fallback: first 1–2 non-numeric sentences
        fallback = [s for s in sents if not too_numeric(s)]
        clean = fallback[:max_bullets]

    return clean[:max_bullets]

def within_since_days(dt_iso: Optional[str], since_days: Optional[int]) -> bool:
    if not since_days: return True
    if not dt_iso: return True
    try:
        dt_parsed = dtparse.parse(dt_iso)
    except Exception:
        return True
    if dt_parsed.tzinfo is None:
        dt_parsed = dt_parsed.replace(tzinfo=timezone.utc)
    else:
        dt_parsed = dt_parsed.astimezone(timezone.utc)
    cutoff = datetime.now(timezone.utc) - timedelta(days=since_days)
    return dt_parsed >= cutoff

# ----------------------------
# Optional OpenAI
# ----------------------------
def ai_summarize_batch(items: List[Dict[str, Any]], model: str, temperature: float = 0.2, max_tokens: int = 400) -> None:
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("[AI] OPENAI_API_KEY not set: skipping AI summaries.")
        return
    try:
        from openai import OpenAI
    except Exception as e:
        print(f"[AI] OpenAI SDK not available ({e}): pip install openai; skipping AI.")
        return

    client = OpenAI(api_key=api_key)
    system_prompt = (
        "You are an experienced investment analyst. Write a crisp, factual, value-investor–oriented summary. "
        "Prioritise: capital allocation (buybacks/returns), balance sheet (net debt/leverage/covenants), "
        "unit economics/margins/cash conversion, major contracts/customer concentration, guidance/inflection, "
        "and risks (liquidity, dilution, going concern). Avoid hype. 4–6 sentences max."
    )

    for idx, r in enumerate(items, 1):
        body = r.get("body") or ""
        title = r.get("title") or ""
        company = r.get("company") or ""
        meta = " | ".join(m for m in [r.get("dt_iso") or "", r.get("source") or "", company] if m)
        excerpt = body[:6000]
        user_msg = f"TITLE: {title}\nMETA: {meta}\nURL: {r.get('url')}\n\nANNOUNCEMENT TEXT:\n{excerpt}\n\nWrite the summary now."
        try:
            resp = client.chat.completions.create(
                model=model,
                temperature=temperature,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_msg},
                ],
                max_tokens=max_tokens,
            )
            r["ai_summary"] = (resp.choices[0].message.content or "").strip()
            if idx % 10 == 0:
                print(f"[AI] Summarised {idx}/{len(items)}")
        except Exception as e:
            print(f"[AI] Failed to summarise one item: {e}")
            r["ai_summary"] = None
        time.sleep(0.3)

# ----------------------------
# Main
# ----------------------------
def run(pages: int, per_page: int, since_days: Optional[int], min_score: int,
        keywords_csv: str, keywords_file: Optional[str], out_dir: str, throttle: float,
        use_ai: bool, openai_model: str):

    user_keywords = load_keywords(keywords_csv, keywords_file)
    user_kw_patterns = compile_phrase_patterns(user_keywords)
    trigger_patterns = [re.compile(rx, flags=re.IGNORECASE) for rx in BUILTIN_INVESTOR_TRIGGERS]

    session = requests.Session()
    session.headers.update(DEFAULT_HEADERS)

    today_str = datetime.now().strftime("%Y-%m-%d")
    out_base = pathlib.Path(out_dir) / today_str
    ensure_dir(out_base)

    # List pages
    all_rows: List[Dict[str, Any]] = []
    for page in range(1, pages + 1):
        list_url = f"https://www.investegate.co.uk/?perPage={per_page}&page={page}"
        html_text = fetch(list_url, session)
        if not html_text:
            print(f"[WARN] Failed to fetch list page {page}")
            continue
        rows = parse_list_page(html_text)
        existing = {r.get("href") for r in all_rows}
        new_rows = [r for r in rows if r["href"] not in existing]
        all_rows.extend(new_rows)
        print(f"[INFO] Page {page}: got {len(new_rows)} new announcements (total {len(all_rows)})")

    # Detail pages
    results = []
    for i, row in enumerate(all_rows, 1):
        url = row["href"]
        html_text = fetch(url, session)
        if not html_text:
            print(f"[WARN] Failed to fetch detail: {url}")
            continue

        detail = parse_detail_page(html_text)
        dt_iso = detail["dt_iso"]
        if not dt_iso and row.get("time_str"):
            try:
                dt_iso = dtparse.parse(row["time_str"], dayfirst=True).isoformat()
            except Exception:
                dt_iso = None

        title = detail["title"] or row.get("headline") or ""
        body = detail["body"]
        teaser = (title + "\n" + (body or "")[:2000])

        # Scores
        user_score = count_matches(teaser, user_kw_patterns) if user_kw_patterns else 0
        trigger_score = count_matches(teaser, trigger_patterns)
        total_score = user_score + trigger_score

        bullets = extract_bullets(body or "", user_kw_patterns, max_bullets=2)

        item = {
            "url": url,
            "title": title,
            "company": row.get("company", ""),
            "source": row.get("source", ""),
            "headline": row.get("headline", ""),
            "listed_time_str": row.get("time_str", ""),
            "dt_iso": dt_iso,
            "user_score": user_score,
            "trigger_score": trigger_score,
            "score": total_score,
            "bullets": bullets,
            "body": body,
        }

        if within_since_days(dt_iso, since_days):
            results.append(item)

        if throttle: time.sleep(throttle)
        if i % 25 == 0:
            print(f"[INFO] Processed {i}/{len(all_rows)}")

    # ===== STRICT FILTERING =====
    # "visible" = ONLY items with user_score >= 1 (and optionally score >= min_score)
    visible = [r for r in results if r.get("user_score", 0) >= 1 and (r.get("score", 0) >= min_score)]

    # AI ONLY for items with user_score >= 1
    if use_ai and visible:
        print(f"[AI] Summarising user-matched items: {len(visible)} (model: {openai_model})")
        ai_summarize_batch(visible, model=openai_model)

    # ----------------- Write outputs -----------------
    # Raw JSON: all results (for debugging/reference)
    raw_path = out_base / "investegate_raw.json"
    with raw_path.open("w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    # CSV: only visible (user_score >= 1)
    hits_path = out_base / "investegate_hits.csv"
    with hits_path.open("w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["dt_iso", "source", "company", "title", "score", "user_score", "trigger_score", "url"])
        for r in sorted(visible, key=lambda x: (x["score"], x.get("dt_iso") or ""), reverse=True):
            w.writerow([r["dt_iso"] or "", r["source"], r["company"], r["title"], r["score"], r["user_score"], r["trigger_score"], r["url"]])

    # Markdown: only visible
    md_path = out_base / "investegate_summary.md"
    def fmt_dt(s: Optional[str]) -> str:
        try:
            return dtparse.parse(s).strftime("%Y-%m-%d %H:%M")
        except Exception:
            return s or ""
    visible_sorted = sorted(visible, key=lambda x: (x["score"], x.get("dt_iso") or ""), reverse=True)
    with md_path.open("w", encoding="utf-8") as f:
        f.write(f"# Investegate — Filtered RNS Digest ({datetime.now().strftime('%Y-%m-%d')})\n\n")
        f.write(f"_Pages scanned: {pages} × {per_page} • Min score: {min_score}_\n\n")
        if user_keywords: f.write(f"_Keywords: {', '.join(user_keywords)}_\n\n")
        f.write(f"_AI summaries: {'ON' if use_ai else 'OFF'} (Only your matches shown)\n\n")
        if not visible_sorted:
            f.write("> No announcements matched your phrases.\n")
        for r in visible_sorted:
            f.write(f"## {r['title']}\n")
            meta = " | ".join(filter(None, [fmt_dt(r.get('dt_iso')), r.get('source'), r.get('company')]))
            f.write(f"{meta}\n\n")
            f.write(f"_Score: {r['score']} (you:{r['user_score']}, trig:{r['trigger_score']})_\n\n")
            for b in r["bullets"]:
                f.write(f"- {b}\n")
            if r.get("ai_summary"):
                f.write(f"\n**AI summary:** {r['ai_summary']}\n")
            f.write(f"\n[Read the announcement]({r['url']})\n\n---\n\n")

    # Self-contained mini-site: ONLY visible rows
    site_dir = out_base / "site"
    ensure_dir(site_dir)
    payload = {
        "stats": {
            "date": datetime.now().strftime("%Y-%m-%d"),
            "pages": pages,
            "per_page": per_page,
            "min_score": min_score,
            "keywords": ", ".join(user_keywords) if user_keywords else "",
            "ai_enabled": bool(use_ai),
            "visible_count": len(visible),
        },
        "rows": visible,
    }
    html_with_data = HTML_TEMPLATE.replace(
        "</head>",
        '<script id="data-json" type="application/json">' +
        json.dumps(payload, ensure_ascii=False) +
        "</script>\n</head>"
    )
    with (site_dir / "index.html").open("w", encoding="utf-8") as f:
        f.write(html_with_data)
    with (site_dir / "styles.css").open("w", encoding="utf-8") as f:
        f.write(CSS_STYLES)

    print(f"[DONE] Wrote (ONLY user matches in CSV/MD/website):\n  {raw_path}\n  {hits_path}\n  {md_path}\n  {site_dir / 'index.html'}")

def main():
    ap = argparse.ArgumentParser(description="Investegate scraper — STRICT (only your matches are shown)")
    ap.add_argument("--pages", type=int, default=2)
    ap.add_argument("--per_page", type=int, default=300, choices=[50,100,200,300])
    ap.add_argument("--since_days", type=int, default=None)
    ap.add_argument("--min_score", type=int, default=1, help="(user+trigger) score for ordering; visibility still requires user_score>=1")
    ap.add_argument("--keywords", type=str, default="")
    ap.add_argument("--keywords_file", type=str, default=None)
    ap.add_argument("--out", type=str, default="out")
    ap.add_argument("--throttle", type=float, default=SLEEP_BETWEEN_ITEM_SEC)
    ap.add_argument("--use_ai", type=str, default="false", help="true/false to enable AI")
    ap.add_argument("--openai_model", type=str, default=os.getenv("OPENAI_MODEL", "gpt-4o"))
    args = ap.parse_args()

    use_ai = str(args.use_ai).strip().lower() in ("1","true","yes","on")

    run(
        pages=args.pages,
        per_page=args.per_page,
        since_days=args.since_days,
        min_score=args.min_score,
        keywords_csv=args.keywords,
        keywords_file=args.keywords_file,
        out_dir=args.out,
        throttle=max(0.2, args.throttle),
        use_ai=use_ai,
        openai_model=args.openai_model,
    )

if __name__ == "__main__":
    main()
