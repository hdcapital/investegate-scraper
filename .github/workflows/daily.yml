name: Daily Investegate Scrape

on:
  schedule:
    # GitHub uses UTC. 06:30 UTC ≈ 5:30pm Sydney during AEDT.
    # If Sydney switches to AEST (UTC+10), change this to '30 07 * * *'.
    - cron: '30 06 * * *'
  workflow_dispatch:  # lets you run it manually

jobs:
  run:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 lxml python-dateutil openai

      - name: Run scraper
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}  # keep AI ON
        run: |
          python investegate_scraper.py \
            --pages 2 \
            --per_page 300 \
            --since_days 7 \
            --min_score 1 \
            --keywords_file keywords.txt \
            --use_ai true

      # Find latest out/<date>/site/index.html
      - name: Locate latest index.html
        id: find_html
        run: |
          set -e
          latest_dir="$(ls -td out/* 2>/dev/null | head -1)"
          if [ -z "$latest_dir" ] || [ ! -f "$latest_dir/site/index.html" ]; then
            echo "No index.html found under out/*/site/" >&2
            exit 1
          fi
          echo "INDEX_FILE=$latest_dir/site/index.html" >> "$GITHUB_ENV"
          echo "Latest HTML: $latest_dir/site/index.html"

      # Minify + cap HTML so the email input isn't too large
      - name: Prepare inline HTML (minify + cap)
        id: html
        shell: bash
        run: |
          set -e
          python - << 'PY'
          import os, re, html
          path = os.environ['INDEX_FILE']
          with open(path, 'r', encoding='utf-8', errors='ignore') as f:
              s = f.read()
          # strip HTML comments
          s = re.sub(r'<!--.*?-->', '', s, flags=re.S)
          # collapse whitespace between tags and long runs
          s = re.sub(r'>\s+<', '><', s)
          s = re.sub(r'\s{2,}', ' ', s)
          MAX = 90000  # ~90 KB inline to avoid GitHub arg limits
          clipped = len(s) > MAX
          if clipped:
              s = s[:MAX] + '...<!-- clipped for email; full HTML attached -->'
              note = ('<div style="padding:12px;border:1px solid #ddd;'
                      'background:#fffbe6;margin:0 0 12px 0;">'
                      'Email preview truncated. Full report attached as index.html.'
                      '</div>')
              s = note + s
          with open('email_inline.html','w',encoding='utf-8') as f:
              f.write(s)
          PY
          {
            echo 'content<<EOF'
            cat email_inline.html
            echo 'EOF'
          } >> "$GITHUB_OUTPUT"

      # Email with inline HTML + attach the full HTML file
      - name: Email report
        uses: dawidd6/action-send-mail@v3
        with:
          server_address: ${{ secrets.SMTP_SERVER }}
          server_port: ${{ secrets.SMTP_PORT }}
          secure: false                  # 587 = STARTTLS; set true if you switch to 465
          username: ${{ secrets.SMTP_USERNAME }}
          password: ${{ secrets.SMTP_PASSWORD }}
          subject: "Investegate RNS Digest – run #${{ github.run_number }}"
          to: ${{ secrets.TO_EMAIL }}
          from: "${{ secrets.FROM_NAME }} <${{ secrets.SMTP_USERNAME }}>"
          html_body: ${{ steps.html.outputs.content }}
          attachments: ${{ env.INDEX_FILE }}

      # Keep artifact upload for backup/downloads
      - name: Upload output
        uses: actions/upload-artifact@v4
        with:
          name: investegate-out
          path: out/
